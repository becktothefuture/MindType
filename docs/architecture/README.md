# MindType Architecture Overview

This document expands on the engineering spec and explains how the parts of the tool fit together. It is designed to provide a mental picture of the final system before implementation begins.

Crossâ€‘links:

- Principles: `../system_principles.md`
- ADRs: `../adr/README.md`
- Guides (reference contracts): `../guide/reference/`
- QA acceptance: `../qa/acceptance/`

## High-Level Pipeline (v0.4)

1. **Keystroke Handling** â€“ Every printable key resets the pause timer and advances a typing tick (~60â€“90 ms cadence) for streamed diffusion.
2. **Fragment Extraction** â€“ The active fragment is the sentence behind the caret within 250 characters (Â± context). Diffusion operates within a trailing band of ~3â€“8 words.
3. **Dualâ€‘Context LM/Rules Correction** â€“ A sentenceâ€‘based, dualâ€‘context strategy drives semantic fixes:
   - Close Context: 2â€“5 sentences surrounding the caret (active sentence excluded, prefix up to the caret included).
   - Wide Context: wholeâ€‘document summary for coherence checks and validation.
     Onâ€‘device language models (Transformers.js + Qwen2.5â€‘0.5Bâ€‘Instruct, q4, WebGPU/WASM) run in a Web Worker via a coreâ€‘owned adapter, with graceful fallback to ruleâ€‘based fixes.
4. **Incremental Diff and Merge** â€“ Patches are caretâ€‘safe and wordâ€‘bounded. During typing, a frontier advances toward the caret; on pause (~500 ms), diffusion catches up.
5. **Injection** â€“ Apply in place, preserving formatting, undo grouping, and cursor position. Visuals: subtle shimmer band; reducedâ€‘motion fallback.

```
key press â†’ [PauseTimer] â†’ idle
           â†“                          â†˜
  [FragmentExtractor]           [Abort stream if new key]
           â†“                          â†˜
 [ContextTransformer]
     â”‚  (band select + prompt)
     â–¼
 [LM Context Manager] â”€â”€ builds { close, wide } windows â†’
     â–¼
 [LM Worker (Transformers.js)] â†’ token stream â†’ [MergeEngine] â†’ patches â†’ [Injector]
```

The arrows illustrate how a typing pause triggers the fragment extractor. Streaming can be aborted if a new key arrives mid-flight. This diagram mirrors both the browser and macOS implementations.

This pipeline is **implemented in Rust** (`crates/core-rs`) and surfaced to each platform via generated bindings. A small TypeScript `DiffusionController` orchestrates streaming ticks and visuals while delegating heavy lifting to the core. In v0.4, LM orchestration is coreâ€‘owned inside the Context stage and runs in a Web Worker on the web. For browser demos, a TypeScriptâ€‘first pipeline is used immediately, with Rust WASM integrated as it lands:

- **Web** â†’ TypeScript streaming pipeline now; WebAssembly package `@mindtype/core` to augment as Rust components land.
- **macOS** â†’ Static library `libmindtype.a` + Swift module created with `cbindgen`.

Maintaining one canonical codebase removes divergence between TypeScript and Swift implementations that were planned in the earlier draft.

## Implementation Phasing (v0.4 â†’ v0.5): TS-first â†’ Rust WASM

- Current (v0.4): TypeScript streaming pipeline runs end-to-end for the web demo (fast iteration, rich tooling).
- Ready: Rust core is compiled to WASM and staged at `crates/core-rs/bindings/wasm/pkg`.
- Plan: Progressive handoff of algorithmic components to Rust WASM under a feature flag while keeping the same TS host/orchestrator:
  - Pause timer, fragment extractor, merge engine â†’ `@mindtype/core` WASM exports
  - LM orchestration remains workerized; Rust owns scheduling/gating; TS stays the host/injector
- Feature flag: `USE_WASM_CORE` (default: off) gates the Rust path to enable A/B rollout.
- Parity & CI: Test suite runs in both modes (TS-only, WASM-enabled) with a golden corpus to ensure behavioral equivalence before flipping the default.
- Not doing it twice: TS continues to orchestrate UI and apply diffs; Rust replaces inner engines behind stable interfaces (`DiffusionController`, `LMAdapter`, `contextTransform`).

## Module Breakdown

### crates/core-rs ðŸ”¹

The Rust crate contains the reference implementations of the pause timer, fragment extractor, merge engine and streaming LLM client. The TS and Swift layers import these functions rather than re-implementing them.

### bindings/wasm ðŸ”¹

Generated by `wasm-bindgen`, this npm package exposes the Rust API to TypeScript with zero-copy string sharing where supported.

### bindings/swift ðŸ”¹

A `module.modulemap` and C header expose the same API to Swift/Obj-C. Build scripts in `mac/` link `libmindtype.a` automatically.

### web-demo

React components wrap the core logic and provide a simple typing playground. It demonstrates streaming corrections in real time and exposes a Workbench for logs/metrics. The LM runs in a module Worker; ONNX Runtime Web assets are served via CDN by default, with optional local `/wasm/` fallback.

### mac/

Native macOS layer written in Swift/SwiftUI. It links to the **same Rust core** via FFI; no re-implementation required.

## System Map & Contracts (authoritative)

The following contracts define how parts communicate efficiently. See linked guides in `docs/guide/reference/**` for detailed specs.

1. Input monitor â†’ Scheduler

- Event: `{ text: string; caret: number; atMs: number }`
- Cadence: typing tick ~60â€“90 ms; pause â‰¥ SHORT_PAUSE_MS (300 ms)
- Abort rule: any new input cancels pending LM work

2. Scheduler â†’ DiffusionController

- Methods: `update({text, caret})`, `tickOnce()`, `catchUp()`
- Invariants: never edits at/after caret; render range throttled to 16 ms

3. DiffusionController â†’ Transformers (Noise/Context/Tone)

- Noise: synchronous `noiseTransform({text, caret}) â†’ {diff|null}`
- Denoising API: async `denoise(text: string) â†’ Promise<string>` for comprehensive fuzzy text correction (testing/integration)
- Context: async `contextTransform({text, caret}, lmAdapter, contextManager) â†’ {proposals[]}`
- Tone: planned `toneTransform({text, caret, target}) â†’ {proposals[]}`
- All proposals must be strictly within active region and â‰¤ caret

4. LMContextManager (dual-context)

- API: `initialize`, `updateWideContext`, `updateCloseContext`, `getContextWindow`, `validateProposal`
- Window policy: close = Â±N sentences around caret (Nâˆˆ[2,5]); wide = full document snapshot with token estimate
- Validation: length ratio â‰¤ 3Ã—; contextual ratio > 0.1; plain-text only

5. LMAdapter (streaming)

- API: `init() â†’ LMCapabilities`, `stream({text, caret, band, settings}) â†’ AsyncIterable<string>`, optional `abort()` and `getStats()`
- Device tiers: WebGPU â†’ WASM â†’ CPU; token caps and cooldowns per tier
- Output discipline: plain text; sanitized; bandâ€‘bounded

6. Merge Policy & Confidence/Staging

- Confidence: compute 4â€‘dimensional score; thresholds Ï„_input, Ï„_commit, Ï„_tone, Ï„_discard
- StagingBuffer states: HOLD â†’ COMMIT â†’ DISCARD; ROLLBACK on caret entry
- Apply order: rules > LM on structural conflicts; LM > rules on semantics

7. Injector & UI feedback

- Apply diff via `replaceRange` (UTFâ€‘16 safe; never crosses caret)
- Events: `mindtype:activeRegion`, `mindtype:highlight`; a11y live region announcements; reducedâ€‘motion â†’ instant swaps

8. Safety & privacy gates (always on)

- Secure fields and IME composition block transforms
- Localâ€‘first by default; remote only with explicit optâ€‘in

Crossâ€‘references:

- Contracts: `guide/reference/{band-policy.md,lm-behavior.md,injector.md,three-stage-pipeline.md,confidence-system.md}`
- Types: `core/lm/types.ts`, `core/lm/contextManager.ts`
- Policies: `config/defaultThresholds.ts`

## Rationale

- **One Pipeline** â€“ By designing a single languageâ€‘agnostic algorithm we avoid divergence between platforms and ensure consistent user experience.
- **Streaming** â€“ Token streaming keeps latency perceptibly low and makes the tool feel alive. This also reduces the risk of large diff conflicts.
- **Local Model Path** â€“ Shipping an onâ€‘device model guarantees privacy and offline usage. The spec outlines the conversion of a small BART model into Core ML as a first milestone.

Further details on specific components can be found in the accompanying documents.

## Next Steps

1. Publish the `@mindtype/core` WASM package to npm once CI is green.
2. Finish FFI bindings in the mac app and verify parity with the Playwrightâ€‰/â€‰XCUITest suite.
3. Run performance tuning and finalise Core ML model conversion.

This overview aims to answer **why** each component exists before diving into code. The shared pipeline enforces consistent behaviour, while individual modules stay small enough to be unit tested in isolation. Developers should be able to run the core on its own (node-based tests) or through the demo/mac frontâ€‘ends without rewriting logic.

The additional documents referenced in the main spec â€“ including [web_demo_details.md](web_demo_details.md) and [mac_app_details.md](mac_app_details.md) â€“ provide stepâ€‘byâ€‘step guidance on implementation choices.

### References (v0.4 LM components)

- `engines/contextTransformer.ts` â€“ LM orchestration lives here (band selection, prompting, merge gating)
- `core/lm/contextManager.ts` â€“ Dualâ€‘context (close + wide) window management
- `core/lm/workerAdapter.ts` â€“ Robust Worker adapter (timeouts, error propagation)
- `core/lm/transformersRunner.ts` â€“ ONNX Runtime Web configuration (CDN/local wasmPaths)
